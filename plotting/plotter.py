import os
import pickle
import re
from typing import List, Tuple, Dict, Union
import gin
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf
from tf_agents.environments.py_environment import PyEnvironment
from tf_agents.trajectories.trajectory import Trajectory

plt.style.use('seaborn-darkgrid')


class Plotter(object):
    """
    This class plots different graphs for a single epoch from
    a list of trajectories generated by eval
    """

    def __init__(self):
        self.observations = []
        self.rewards = []
        self.actions = []
        self.ratio = []
        self.theta = []
        self.timesteps = 0

        # initialise some dicts used to easy plotting
        self.color_dict, self.label_dict = RegexDict(), RegexDict()
        colors = ['mediumblue', 'dodgerblue', 'firebrick', 'orange', 'green']
        labels = ['DDPG_baseline', 'DDPG_hybrid', 'PILCO_baseline', 'PILCO_hybrid', 'linear']
        controller_regex = ['.*ddpg_baseline.*', '.*ddpg_hybrid.*', '.*pilco_baseline.*', '.*pilco_hybrid.*', '.*linear.*']
        for i in range(len(controller_regex)):
            self.color_dict[controller_regex[i]] = colors[i]
            self.label_dict[controller_regex[i]] = labels[i]

    def __call__(self, trajectories: List[Trajectory], **kwargs) -> None:
        raise NotImplementedError

    def _helper(self, trajectories: List[Trajectory], num_episodes: int = 1) -> None:
        self.trajectories = trajectories
        self.observations = [x.observation for x in trajectories]
        self.rewards = [x.reward for x in trajectories]
        self.actions = [x.action for x in trajectories]
        self.ratio = [x.policy_info for x in trajectories]
        self.timesteps = int(len(self.observations) / num_episodes)

    def traj2obs(self) -> np.ndarray:
        """
        Input:
            trajectories: trajectories from eval
        Output:
            np.ndarray of state observations
        """
        observations = [i.numpy()[0] for i in self.observations]
        observations = np.array(observations[:self.timesteps])
        return observations

    def traj2theta(self, obs_idx: int, acos: bool = False, asin: bool = False) -> np.ndarray:
        """
        Input:
            trajectories: trajectories from eval
            obs_idx: index of the observation corresponding to cosine or sine
        Output:
            np.ndarray of states
        """
        observations = self.traj2obs()
        theta = observations[:self.timesteps, obs_idx]

        assert not (acos and asin), '--- Error: Both acos and asin are True! ---'
        if asin:
            theta = np.arcsin(theta)
            theta = theta / np.pi * 180
        elif acos:
            theta = np.arccos(theta)
            theta = theta / np.pi * 180
        self.theta = theta
        return theta

    def traj2info(self) -> np.ndarray:
        """
        Input:
            trajectories: trajectories from eval
        Output:
            np.ndarray of info/controller ratios
        """
        try:
            ratios = [i.numpy()[0] for i in self.ratio]
        except AttributeError:
            ratios = self.ratio
        ratios = np.array(ratios[:self.timesteps])
        return ratios

    def _ss(self, ss=0.0):
        sample1 = self.theta[-5:]
        sample2 = self.theta[-6:-1]
        ss_reached = np.allclose(sample1, sample2, rtol=0, atol=1e-03)
        return ss_reached, self.theta[-1] - ss

    def _overshoot(self, ss_reached: bool) -> float:
        # overshoot only applicable if ss reached
        if ss_reached:
            # assume observations start at 0
            overshoot = max(np.max(self.theta), 0.0)
            if not np.isclose(overshoot, self.theta[-1], rtol=0, atol=1e-03):
                return overshoot - self.theta[-1]
            else:
                return 0.0
        else:
            return None

    def _undershoot(self, ss_reached: bool) -> float:
        # undershoot only applicable if ss reached
        if ss_reached:
            # assume observations start at 0
            undershoot = min(np.min(self.theta), 0.0)
            if not np.isclose(undershoot, self.theta[-1], rtol=0, atol=1e-03):
                return self.theta[-1] - undershoot
            else:
                return 0.0
        else:
            return None

    def _rise_time(self, percentage=0.9):
        ss_reached, ss_error = self._ss()
        if ss_reached and ss_error != 0.0:
            if ss_error > 0:
                return np.argmax(self.theta > percentage * ss_error)
            else:
                return np.argmax(self.theta < percentage * ss_error)
        else:
            return None

    def _overshoot_peak_time(self, ss_reached: bool):
        if self._overshoot(ss_reached) is None or self._overshoot(ss_reached) == 0.0:
            return None
        else:
            return self.theta.index(max(self.theta))

    def _undershoot_peak_time(self, ss_reached: bool):
        if self._overshoot(ss_reached) is None or self._undershoot(ss_reached) == 0.0:
            return None
        else:
            return self.theta.index(min(self.theta))

    def _settle_time(self, ss_target, percentage=0.1) -> int:
        theta_reverse = self.theta[::-1]
        if np.isclose(ss_target, 0.0, rtol=0, atol=1e-03):
            return None
        elif ss_target > 0:
            upper_stability_bound, lower_stability_bound = ss_target * percentage, ss_target * (1.0 - percentage)
        else:
            upper_stability_bound, lower_stability_bound = ss_target * (1.0 - percentage), ss_target * percentage

        settling_time = np.argmax(lower_stability_bound < theta_reverse < upper_stability_bound)
        settling_time = self.timesteps - settling_time
        return settling_time

    def _obtain_metrics(self):
        """
        Output:
            np.ndarray of the control theory metrics
        """
        # TODO: test and debug the other control metrics
        return self._ss()


@gin.configurable
class StatePlotter(Plotter):
    # Plot the state theta and ratio for one evaluation in one env
    def __init__(self, env: PyEnvironment):
        super().__init__()
        self._init_env(env)

    def _init_env(self, env: PyEnvironment):
        self.env_name = env.unwrapped.spec.id[:-3]
        # Depending on the env, cos(theta) or state of interest is located at different obs_idx
        if self.env_name == 'Pendulum':
            self.acos = True
            self.asin = False
            self.obs_idx = 0
        elif self.env_name == 'Cartpole':
            self.acos = True
            self.asin = False
            self.obs_idx = 2
        elif self.env_name == 'Mountaincar':
            self.acos = False
            self.asin = False
            self.obs_idx = 0
        else:
            raise Exception(f'--- Error: Wrong env in {self} ---')

    def __call__(self, trajectories: List[Trajectory], num_episodes: int = 1) -> None:
        super()._helper(trajectories, num_episodes)

        thetas = self.traj2theta(obs_idx=self.obs_idx, acos=self.acos)
        ratios = self.traj2info()

        fig, ax1 = plt.subplots(figsize=(6, 4))

        # Plot theta
        ln1 = ax1.plot(np.arange(self.timesteps), thetas, color='royalblue', label='\u03B8')
        ax1.set_ylim(0, 180)
        ax1.set_xlabel('Timesteps')
        ax1.set_ylabel(f'\u03B8 (\u00B0)')

        # Plot linear ratio
        ax2 = ax1.twinx()
        ln2 = ax2.plot(np.arange(self.timesteps), ratios, color='darkorange', label='Controller Ratio')
        ax2.set_ylim(0.0, 1.0)
        ax2.set_ylabel(f'Linear Controller Ratio')
        ax2.grid(False)

        # Set legend and format
        lns = ln1 + ln2
        labs = [l.get_label() for l in lns]
        ax1.legend(lns, labs, loc=7)
        ax1.set_title(f'\u03B8 and Linear Controller Ratio')
        fig.show()


@gin.configurable
class ControlMetricsPlotter(Plotter):
    # Plot the impulse and step response of all environments, with respective controllers
    def __init__(self, envs_names: List[str]):
        super().__init__()
        self.envs_names = envs_names
        self.graphs_num = len(envs_names)

    def _init_env(self, env_name: str):
        self.env_name = env_name
        # Depending on the env, sin(theta) or state of interest is located at different obs_idx
        if self.env_name == 'Pendulum':
            self.asin = True
            self.acos = False
            self.obs_idx = 1
        elif self.env_name == 'Cartpole':
            self.asin = True
            self.acos = False
            self.obs_idx = 3
        elif self.env_name == 'Mountaincar':
            self.asin = False
            self.acos = False
            self.obs_idx = 0
        else:
            raise Exception(f'--- Error: Wrong env in {self} ---')

    def __call__(self, all_trajectories_list: List[dict], num_episodes: int = 1) -> None:
        '''
        Input:
            all_trajectories_list: a list of dict of trajectories for all envs
            num_episodes: number of evaluative episodes
        Output:
            control theory metrics
        '''
        fig, axs = plt.subplots(len(all_trajectories_list), self.graphs_num, figsize=(self.graphs_num * 4, 5))

        for j, all_trajectories in enumerate(all_trajectories_list):
            for i, env_name in enumerate(self.envs_names):
                self._init_env(env_name)

                cur_axis = axs[j][i]
                lns, labs = [], []

                for controller in all_trajectories[self.env_name]:
                    # all_trajectories[self.env_name] contains all the trajectories for the env
                    trajectories = all_trajectories[self.env_name][controller]
                    for trajectory in trajectories:
                        super()._helper(trajectory, num_episodes)
                        thetas = self.traj2theta(obs_idx=self.obs_idx, asin=self.asin)

                        # Plot theta
                        ln = cur_axis.plot(np.arange(self.timesteps), thetas, color=self.color_dict[controller],
                                           alpha=1.0, label=self.label_dict[controller])

                        metrics = self._obtain_metrics()
                        print(f'{controller} {metrics}')

                        lns = lns + ln

                # Set legend and format
                cur_axis.set_xlabel('Timesteps')
                if i == 0 and j == 0:
                    cur_axis.set_ylabel(f'Impulse response \u03B8 (\u00B0)')
                elif i == 0 and j == 1:
                    cur_axis.set_ylabel(f'Step response \u03B8 (\u00B0)')
                # if env_name == 'Pendulum' or env_name == 'Cartpole':
                #     cur_axis.set_ylim(-2.0, 2.0)
                # elif env_name == 'Mountaincar':
                #     cur_axis.set_ylim(-0.05, 0.05)
                labs = [l.get_label() for l in lns]
                cur_axis.legend(lns, labs)
                if j == 0:
                    cur_axis.set_title(f'{self.env_name}')

        fig.show()


@gin.configurable
class RobustnessPlotter(Plotter):

    def __init__(self, env: PyEnvironment):
        super().__init__()
        self.env_name = env.unwrapped.spec.id[:-3]
        # Depending on the env, sin(theta) or state of interest is located at different obs_idx
        if self.env_name == 'Pendulum':
            self.asin = True
            self.acos = False
            self.obs_idx = 1
        elif self.env_name == 'Cartpole':
            self.asin = True
            self.acos = False
            self.obs_idx = 3
        elif self.env_name == 'Mountaincar':
            self.asin = False
            self.acos = False
            self.obs_idx = 0
        else:
            raise Exception(f'--- Error: Wrong env in {self} ---')

    def __call__(self,
                 dict_of_dict_controller_name_scaling_trajectory: Dict[str, Dict[float, List[Trajectory]]]) -> None:
        fig, ax = plt.subplots(figsize=(6, 4))
        lns, labs = [], []
        colors = ['mediumblue', 'dodgerblue', 'firebrick', 'orange', 'green']
        labels = ['DDPG_baseline', 'DDPG_hybrid', 'PILCO_baseline', 'PILCO_hybrid', 'linear_ctrl']

        for i, (controller_name, dict_scaling_trajectory) in enumerate(
                dict_of_dict_controller_name_scaling_trajectory.items()):
            scale, overshoot, rise, settle = [], [], [], []
            for scaling, trajectory in dict_scaling_trajectory.items():
                self._helper(trajectory)
                theta = self.traj2theta(obs_idx=self.obs_idx, asin=self.asin)
                overshoot, rising_time, settling_time = self._obtain_metrics(theta)
                scale.append(scaling)
                overshoot.append(overshoot)
                rising_time.append(rising_time)
                settling_time.append(settling_time)
            ln = ax.plot(scale, rise, color=colors[i % len(colors)], label=controller_name)

        ax.set_xlabel('Timesteps')
        ax.set_ylabel(f'\u03B8 (\u00B0)')

        # Set legend and format
        labs = [l.get_label() for l in lns]
        ax.legend(lns, labs)
        ax.set_title(f'{self.env_name}')
        fig.show()
        pass


class LearningCurvePlotter(object):
    """
    This class plots learning curves for the entire training session from
    a list of np.arrays
    """

    def __init__(self):
        pass

    def __call__(self, all_curves: dict) -> None:
        '''
        Input:
            all_curves: a dict of dict of all the rewards
            The content of the dictionary should be structured as such:
            dict[env_name][policy] = AwardCurves object for the env and policy
        '''
        self.envs_names = all_curves.keys()
        self.graphs_num = len(self.envs_names)

        colors = {'ddpg_baseline': 'mediumblue', 'ddpg_hybrid': 'dodgerblue',
                  'pilco_baseline': 'firebrick', 'pilco_hybrid': 'orange'}
        policies = [['ddpg_baseline', 'ddpg_hybrid'], ['pilco_baseline', 'pilco_hybrid']]
        rows = len(policies)

        # Plot graph
        fig, axs = plt.subplots(rows, self.graphs_num, figsize=(self.graphs_num * 4, 5))

        i = 0
        for env_name in self.envs_names:
            for j in range(rows):
                cur_axis = axs[j][i]
                for policy_name in policies[j]:  # all_curves[env_name].keys():
                    if policy_name not in all_curves[env_name]:
                        break
                    curve = all_curves[env_name][policy_name]
                    xs = curve.x
                    # curve.normalise()
                    mean = curve.mean()
                    std = curve.std()
                    color = colors[policy_name]
                    best = curve.best()
                    worst = curve.worst()
                    # Plot learning curve for the current env and policy
                    cur_axis.plot(xs, mean, color=color, label=policy_name)
                    cur_axis.fill_between(xs, mean - std, mean + std, alpha=0.5, facecolor=color,
                                          label=policy_name + ' \u00B1 sd')

                # Set legend and format
                # cur_axis.set_ylim(0.0, 1.0)
                # cur_axis.set_xscale('log')
                cur_axis.set_xlabel('Interaction time (s)')
                if i == 0:
                    cur_axis.set_ylabel('Normalised rewards')
                cur_axis.legend()
                if j == 0:
                    cur_axis.set_title(env_name)
            i += 1

        plt.show()


class RegexDict(dict):

    def __init__(self):
        super().__init__()

    def __getitem__(self, item):
        for k, v in self.items():
            if re.match(k, item):
                return v
        raise KeyError


class AwardCurve(object):
    def __init__(self, data: np.ndarray):
        self.x = data[0]
        self.y = data[1:]

    def append(self, other):
        # Append other to self
        assert len(self.x) == len(other.x), '--- Error: Different timesteps ---'
        # if len(self.x) < len(other.x):
        #     repeats = len(other.x) - len(self.x)
        #     repeat_columns = np.tile(self.y[:, [-1]], repeats)
        #     self.y = np.hstack((self.y, repeat_columns))
        #     self.x = np.hstack((self.x, other.x[-repeats:]))
        # elif len(self.x) > len(other.x):
        #     repeats = len(self.x) - len(other.x)
        #     repeat_columns = np.tile(other.y[-1], repeats)
        #     other.y = np.hstack((other.y, repeat_columns))
        self.y = np.vstack((self.y, other.y))

    def _check_dim(self):
        if self.y.ndim == 1:
            self.y = np.expand_dims(self.y, axis=0)

    def mean(self):
        self._check_dim()
        return np.mean(self.y, axis=0)

    def best(self):
        self._check_dim()
        return np.max(self.y, axis=0)

    def worst(self):
        self._check_dim()
        return np.min(self.y, axis=0)

    def std(self):
        self._check_dim()
        return np.std(self.y, axis=0)

    def normalise(self):
        self.y = (self.y - np.min(self.y)) / (np.max(self.y) - np.min(self.y))
