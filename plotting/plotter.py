import os
import pickle
import re
from typing import List, Tuple, Dict, Union
import gin
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf
from tf_agents.environments.py_environment import PyEnvironment
from tf_agents.trajectories.trajectory import Trajectory

plt.style.use('seaborn-darkgrid')


class Plotter(object):
    """
    This class plots different graphs for a single epoch from
    a list of trajectories generated by eval
    """

    def __init__(self):
        self.observations = []
        self.rewards = []
        self.actions = []
        self.ratio = []
        self.timesteps = 0

    def __call__(self, trajectories: List[Trajectory], **kwargs) -> None:
        raise NotImplementedError

    def _helper(self, trajectories: List[Trajectory], num_episodes: int = 1) -> None:
        self.trajectories = trajectories
        self.observations = [x.observation for x in trajectories]
        self.rewards = [x.reward for x in trajectories]
        self.actions = [x.action for x in trajectories]
        self.ratio = [x.policy_info for x in trajectories]
        self.timesteps = int(len(self.observations) / num_episodes)

    def traj2obs(self) -> np.ndarray:
        """
        Input:
            trajectories: trajectories from eval
        Output:
            np.ndarray of state observations
        """
        observations = [i.numpy()[0] for i in self.observations]
        observations = np.array(observations[:self.timesteps])
        return observations

    def traj2theta(self, obs_idx: int, acos: bool = False, asin: bool = False) -> np.ndarray:
        """
        Input:
            trajectories: trajectories from eval
            obs_idx: index of the observation corresponding to cosine or sine
        Output:
            np.ndarray of states
        """
        observations = self.traj2obs()
        theta = observations[:self.timesteps, obs_idx]

        assert not (acos and asin), '--- Error: Both acos and asin are True! ---'
        if asin:
            theta = tf.math.asin(theta)
        elif acos:
            theta = tf.math.acos(theta)
        theta = theta / np.pi * 180
        return theta

    def traj2info(self) -> np.ndarray:
        """
        Input:
            trajectories: trajectories from eval
        Output:
            np.ndarray of info/controller ratios
        """
        try:
            ratios = [i.numpy()[0] for i in self.ratio]
        except AttributeError:
            ratios = self.ratio
        ratios = np.array(ratios[:self.timesteps])
        return ratios


@gin.configurable
class StatePlotter(Plotter):
    # Plot the state theta and ratio for one evaluation in one env
    def __init__(self, env: PyEnvironment):
        super().__init__()
        self._init_env(env)

    def _init_env(self, env: PyEnvironment):
        self.env_name = env.unwrapped.spec.id[:-3]
        # Depending on the env, cos(theta) or state of interest is located at different obs_idx
        if self.env_name == 'Pendulum':
            self.acos = True
            self.asin = False
            self.obs_idx = 0
        elif self.env_name == 'Cartpole':
            self.acos = True
            self.asin = False
            self.obs_idx = 2
        elif self.env_name == 'Mountaincar':
            self.acos = False
            self.asin = False
            self.obs_idx = 0
        else:
            raise Exception(f'--- Error: Wrong env in {self} ---')

    def __call__(self, trajectories: List[Trajectory], num_episodes: int = 1) -> None:
        super()._helper(trajectories, num_episodes)

        thetas = self.traj2theta(obs_idx=self.obs_idx, acos=self.acos)
        ratios = self.traj2info()

        fig, ax1 = plt.subplots(figsize=(6, 4))

        # Plot theta
        ln1 = ax1.plot(np.arange(self.timesteps), thetas, color='royalblue', label='\u03B8')
        ax1.set_ylim(0, 180)
        ax1.set_xlabel('Timesteps')
        ax1.set_ylabel(f'\u03B8 (\u00B0)')

        # Plot linear ratio
        ax2 = ax1.twinx()
        ln2 = ax2.plot(np.arange(self.timesteps), ratios, color='darkorange', label='Controller Ratio')
        ax2.set_ylim(0.0, 1.0)
        ax2.set_ylabel(f'Linear Controller Ratio')
        ax2.grid(False)

        # Set legend and format
        lns = ln1 + ln2
        labs = [l.get_label() for l in lns]
        ax1.legend(lns, labs, loc=7)
        ax1.set_title(f'\u03B8 and Linear Controller Ratio')
        fig.show()


@gin.configurable
class ControlMetricsPlotter(Plotter):
    # Plot the impulse response of all environments, with respective controllers
    def __init__(self, envs: List[PyEnvironment]):
        super().__init__()
        self.envs = envs
        self.graphs_num = len(envs)

        # initialise some dicts used to easy plotting
        self.color_dict, self.label_dict = RegexDict(), RegexDict()
        colors = ['mediumblue', 'dodgerblue', 'firebrick', 'orange', 'green']
        labels = ['DDPG_baseline', 'DDPG_hybrid', 'PILCO_baseline', 'PILCO_hybrid', 'linear_ctrl']
        controller_regex = ['.*ddpg_baseline.*', '.*ddpg_hybrid.*', '.*pilco_baseline.*', '.*pilco_hybrid.*']
        for i in range(len(controller_regex)):
            self.color_dict[controller_regex[i]] = colors[i]
            self.label_dict[controller_regex[i]] = labels[i]

    def _init_env(self, env: PyEnvironment):
        self.env_name = env.unwrapped.spec.id[:-3]
        # Depending on the env, sin(theta) or state of interest is located at different obs_idx
        if self.env_name == 'Pendulum':
            self.asin = True
            self.acos = False
            self.obs_idx = 1
        elif self.env_name == 'Cartpole':
            self.asin = True
            self.acos = False
            self.obs_idx = 3
        elif self.env_name == 'Mountaincar':
            self.asin = False
            self.acos = False
            self.obs_idx = 0
        else:
            raise Exception(f'--- Error: Wrong env in {self} ---')

    def __call__(self, all_trajectories: dict, num_episodes: int = 1) -> None:
        '''
        Input:
            all_trajectories: a dict of dict of trajectories for all envs
            num_episodes: number of evaluative episodes
        Output:
            control theory metrics
        '''
        fig, axs = plt.subplots(1, self.graphs_num, figsize=(self.graphs_num * 4, 4))

        for i in range(self.graphs_num):
            env = self.envs[i]
            self._init_env(env)

            cur_axis = axs[i]
            lns, labs = [], []

            for controller in all_trajectories[self.env_name]:
                # all_trajectories[self.env_name] contains all the trajectories for the env
                trajectory = all_trajectories[self.env_name][controller]
                super()._helper(trajectory, num_episodes)
                thetas = self.traj2theta(obs_idx=self.obs_idx, asin=self.asin)

                # Plot theta
                ln = cur_axis.plot(np.arange(self.timesteps), thetas, color=self.color_dict[controller],
                                   label=self.label_dict[controller])
                lns = lns + ln

            # Set legend and format
            cur_axis.set_xlabel('Timesteps')
            if i == 0:
                cur_axis.set_ylabel(f'\u03B8 (\u00B0)')
            labs = [l.get_label() for l in lns]
            cur_axis.legend(lns, labs)
            cur_axis.set_title(f'{self.env_name}')

        fig.show()

    def _obtain_metrics(self, target: List[float], stability_bound: float) -> Tuple[float, int, int]:
        """
        Input:
            trajectories: trajectories from eval
        Output:
            np.ndarray of the control theory metrics
        """
        theta = self.traj2theta(obs_idx=self.obs_idx, asin=self.asin)
        peak_overshot = max(abs(theta)) - target
        rising_time = theta.index(max(theta))

        theta_reverse = theta[::-1]
        upper_stability_bound, lower_stability_bound = target + stability_bound, target - stability_bound
        settling_time = np.argmax(lower_stability_bound < theta_reverse < upper_stability_bound)
        settling_time = len(theta_reverse) - settling_time

        return (peak_overshot, rising_time, settling_time)


@gin.configurable
class RobustnessPlotter(Plotter):

    def __init__(self, env: PyEnvironment):
        super().__init__()
        self.env_name = env.unwrapped.spec.id[:-3]
        # Depending on the env, sin(theta) or state of interest is located at different obs_idx
        if self.env_name == 'Pendulum':
            self.asin = True
            self.acos = False
            self.obs_idx = 1
        elif self.env_name == 'Cartpole':
            self.asin = True
            self.acos = False
            self.obs_idx = 3
        elif self.env_name == 'Mountaincar':
            self.asin = False
            self.acos = False
            self.obs_idx = 0
        else:
            raise Exception(f'--- Error: Wrong env in {self} ---')

    def __call__(self,
                 dict_of_dict_controller_name_scaling_trajectory: Dict[str, Dict[float, List[Trajectory]]]) -> None:
        fig, ax = plt.subplots(figsize=(6, 4))
        lns, labs = [], []
        colors = ['mediumblue', 'dodgerblue', 'firebrick', 'orange', 'green']
        labels = ['DDPG_baseline', 'DDPG_hybrid', 'PILCO_baseline', 'PILCO_hybrid', 'linear_ctrl']

        for i, (controller_name, dict_scaling_trajectory) in enumerate(
                dict_of_dict_controller_name_scaling_trajectory.items()):
            scale, overshoot, rise, settle = [], [], [], []
            for scaling, trajectory in dict_scaling_trajectory.items():
                self._helper(trajectory)
                theta = self.traj2theta(obs_idx=self.obs_idx, asin=self.asin)
                overshoot, rising_time, settling_time = self._obtain_metrics(theta)
                scale.append(scaling)
                overshoot.append(overshoot)
                rising_time.append(rising_time)
                settling_time.append(settling_time)
            ln = ax.plot(scale, rise, color=colors[i % len(colors)], label=controller_name)

        ax.set_xlabel('Timesteps')
        ax.set_ylabel(f'\u03B8 (\u00B0)')

        # Set legend and format
        labs = [l.get_label() for l in lns]
        ax.legend(lns, labs)
        ax.set_title(f'{self.env_name}')
        fig.show()
        pass

    def _obtain_metrics(self, theta, target: float = 0, stability_bound: float = (np.pi * 0.1) / 180):
        peak_overshot = max(abs(theta)) - target
        rising_time = theta.index(max(theta))

        theta_reverse = theta[::-1]
        upper_stability_bound, lower_stability_bound = target + stability_bound, target - stability_bound
        settling_time = np.argmax(lower_stability_bound < theta_reverse < upper_stability_bound)
        settling_time = len(theta_reverse) - settling_time
        return (peak_overshot, rising_time, settling_time)


class LearningCurvePlotter(object):
    """
    This class plots learning curves for the entire training session from
    a list of np.arrays
    """

    def __init__(self):
        pass

    def __call__(self, all_curves: dict) -> None:
        '''
        Input:
            all_curves: a dict of dict of all the rewards
            The content of the dictionary should be structured as such:
            dict[env_name][policy] = AwardCurves object for the env and policy
        '''
        self.envs_names = all_curves.keys()
        self.graphs_num = len(self.envs_names)

        # Plot graph
        fig, axs = plt.subplots(1, self.graphs_num, figsize=(self.graphs_num * 4, 4))
        colors = {'ddpg_baseline': 'mediumblue', 'ddpg_hybrid': 'dodgerblue',
                  'pilco_baseline': 'firebrick', 'pilco_hybrid': 'orange'}

        i = 0
        for env_name in self.envs_names:
            cur_axis = axs[i]
            for policy_name in all_curves[env_name].keys():
                curve = all_curves[env_name][policy_name]
                xs = curve.x
                curve.normalise()
                mean = curve.mean()
                std = curve.std()
                color = colors[policy_name]
                best = curve.best()
                worst = curve.worst()
                # Plot learning curve for the current env and policy
                cur_axis.plot(xs, mean, color=color, label=policy_name)
                cur_axis.fill_between(xs, mean-std, mean+std, alpha=0.5, facecolor=color, label=policy_name+' \u00B1 sd')

            # Set legend and format
            cur_axis.set_ylim(0.0, 1.0)
            cur_axis.set_xscale('log')
            cur_axis.set_xlabel('Interaction time (s)')
            if i == 0:
                cur_axis.set_ylabel('Normalised rewards')
            cur_axis.legend()
            cur_axis.set_title(env_name)
            i += 1

        plt.show()


class RegexDict(dict):

    def __init__(self):
        super().__init__()

    def __getitem__(self, item):
        for k, v in self.items():
            if re.match(k, item):
                return v
        raise KeyError


class AwardCurve(object):
    def __init__(self, data: np.ndarray):
        self.x = data[0]
        self.y = data[1:]

    def append(self, other):
        # Append other to self
        assert len(self.x) == len(other.x), '--- Error: Different timesteps ---'
        self.y = np.vstack((self.y, other.y))

    def _check_dim(self):
        if self.y.ndim == 1:
            self.y = np.expand_dims(self.y, axis=0)

    def mean(self):
        self._check_dim()
        return np.mean(self.y, axis=0)

    def best(self):
        self._check_dim()
        return np.max(self.y, axis=0)

    def worst(self):
        self._check_dim()
        return np.min(self.y, axis=0)

    def std(self):
        self._check_dim()
        return np.std(self.y, axis=0)

    def normalise(self):
        self.y = (self.y - np.min(self.y)) / (np.max(self.y) - np.min(self.y))
