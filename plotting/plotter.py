import os
import pickle
import re
from typing import List, Tuple, Dict, Union
import gin
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf
from tf_agents.environments.py_environment import PyEnvironment
from tf_agents.trajectories.trajectory import Trajectory

plt.style.use('seaborn-darkgrid')


class Plotter(object):
    """
    This class plots different graphs for a single epoch from
    a list of trajectories generated by eval
    """

    def __init__(self):
        self.observations = []
        self.rewards = []
        self.actions = []
        self.ratio = []
        self.timesteps = 0

    def __call__(self, trajectories: List[Trajectory], **kwargs) -> None:
        raise NotImplementedError

    def _helper(self, trajectories: List[Trajectory], num_episodes: int = 1) -> None:
        self.trajectories = trajectories
        self.observations = [x.observation for x in trajectories]
        self.rewards = [x.reward for x in trajectories]
        self.actions = [x.action for x in trajectories]
        self.ratio = [x.policy_info for x in trajectories]
        self.timesteps = int(len(self.observations) / num_episodes)

    def traj2obs(self) -> np.ndarray:
        """
        Input:
            trajectories: trajectories from eval
        Output:
            np.ndarray of state observations
        """
        observations = [i.numpy()[0] for i in self.observations]
        observations = np.array(observations[:self.timesteps])
        return observations

    def traj2theta(self, obs_idx: int, acos: bool = False, asin: bool = False) -> np.ndarray:
        """
        Input:
            trajectories: trajectories from eval
            obs_idx: index of the observation corresponding to cosine or sine
        Output:
            np.ndarray of states
        """
        observations = self.traj2obs()
        theta = observations[:self.timesteps, obs_idx]

        assert not (acos and asin), '--- Error: Both acos and asin are True! ---'
        if asin:
            theta = tf.math.asin(theta)
        elif acos:
            theta = tf.math.acos(theta)
        theta = theta / np.pi * 180
        return theta

    def traj2info(self) -> np.ndarray:
        """
        Input:
            trajectories: trajectories from eval
        Output:
            np.ndarray of info/controller ratios
        """
        try:
            ratios = [i.numpy()[0] for i in self.ratio]
        except AttributeError:
            ratios = self.ratio
        ratios = np.array(ratios[:self.timesteps])
        return ratios


@gin.configurable
class StatePlotter(Plotter):
    # Plot the state theta and ratio for one evaluation in one env
    def __init__(self, env: PyEnvironment):
        super().__init__()
        self._init_env(env)

    def _init_env(self, env: PyEnvironment):
        self.env_name = env.unwrapped.spec.id[:-3]
        # Depending on the env, cos(theta) or state of interest is located at different obs_idx
        if self.env_name == 'Pendulum':
            self.acos = True
            self.asin = False
            self.obs_idx = 0
        elif self.env_name == 'Cartpole':
            self.acos = True
            self.asin = False
            self.obs_idx = 2
        elif self.env_name == 'Mountaincar':
            self.acos = False
            self.asin = False
            self.obs_idx = 0
        else:
            raise Exception(f'--- Error: Wrong env in {self} ---')

    def __call__(self, trajectories: List[Trajectory], num_episodes: int = 1) -> None:
        super()._helper(trajectories, num_episodes)

        thetas = self.traj2theta(obs_idx=self.obs_idx, acos=self.acos)
        ratios = self.traj2info()

        fig, ax1 = plt.subplots(figsize=(6, 4))

        # Plot theta
        ln1 = ax1.plot(np.arange(self.timesteps), thetas, color='royalblue', label='\u03B8')
        ax1.set_ylim(0, 180)
        ax1.set_xlabel('Timesteps')
        ax1.set_ylabel(f'\u03B8 (\u00B0)')

        # Plot linear ratio
        ax2 = ax1.twinx()
        ln2 = ax2.plot(np.arange(self.timesteps), ratios, color='darkorange', label='Controller Ratio')
        ax2.set_ylim(0.0, 1.0)
        ax2.set_ylabel(f'Linear Controller Ratio')
        ax2.grid(False)

        # Set legend and format
        lns = ln1 + ln2
        labs = [l.get_label() for l in lns]
        ax1.legend(lns, labs, loc=7)
        ax1.set_title(f'\u03B8 and Linear Controller Ratio')
        fig.show()


@gin.configurable
class ControlMetricsPlotter(Plotter):
    # Plot the impulse response of all environments, with respective controllers
    def __init__(self, envs: List[PyEnvironment]):
        super().__init__()
        self.envs = envs
        self.graphs_num = len(envs)

        # initialise some dicts used to easy plotting
        self.color_dict, self.label_dict = RegexDict(), RegexDict()
        colors = ['mediumblue', 'dodgerblue', 'firebrick', 'orange', 'green']
        labels = ['DDPG_baseline', 'DDPG_hybrid', 'PILCO_baseline', 'PILCO_hybrid', 'linear_ctrl']
        controller_regex = ['.*ddpg_baseline.*', '.*ddpg_hybrid.*', '.*pilco_baseline.*', '.*pilco_hybrid.*']
        for i in range(len(controller_regex)):
            self.color_dict[controller_regex[i]] = colors[i]
            self.label_dict[controller_regex[i]] = labels[i]

    def _init_env(self, env: PyEnvironment):
        self.env_name = env.unwrapped.spec.id[:-3]
        # Depending on the env, sin(theta) or state of interest is located at different obs_idx
        if self.env_name == 'Pendulum':
            self.asin = True
            self.acos = False
            self.obs_idx = 1
        elif self.env_name == 'Cartpole':
            self.asin = True
            self.acos = False
            self.obs_idx = 3
        elif self.env_name == 'Mountaincar':
            self.asin = False
            self.acos = False
            self.obs_idx = 0
        else:
            raise Exception(f'--- Error: Wrong env in {self} ---')

    def __call__(self, all_trajectories: dict, num_episodes: int = 1) -> None:
        '''
        Input:
            all_trajectories: a dict of dict of trajectories for all envs
            num_episodes: number of evaluative episodes
        Output:
            control theory metrics
        '''
        fig, axs = plt.subplots(1, self.graphs_num, figsize=(self.graphs_num * 4, 4))

        for i in range(self.graphs_num):
            env = self.envs[i]
            self._init_env(env)

            cur_axis = axs[i]
            lns, labs = [], []

            for controller in all_trajectories[self.env_name]:
                # all_trajectories[self.env_name] contains all the trajectories for the env
                trajectory = all_trajectories[self.env_name][controller]
                super()._helper(trajectory, num_episodes)
                thetas = self.traj2theta(obs_idx=self.obs_idx, asin=self.asin)

                # Plot theta
                ln = cur_axis.plot(np.arange(self.timesteps), thetas, color=self.color_dict[controller],
                                   label=self.label_dict[controller])
                lns = lns + ln

            # Set legend and format
            cur_axis.set_xlabel('Timesteps')
            if i == 0:
                cur_axis.set_ylabel(f'\u03B8 (\u00B0)')
            labs = [l.get_label() for l in lns]
            cur_axis.legend(lns, labs)
            cur_axis.set_title(f'{self.env_name}')

        fig.show()

    def _obtain_metrics(self, target: List[float], stability_bound: float) -> Tuple[float, int, int]:
        """
        Input:
            trajectories: trajectories from eval
        Output:
            np.ndarray of the control theory metrics
        """
        theta = self.traj2theta(obs_idx=self.obs_idx, asin=self.asin)
        peak_overshot = max(abs(theta)) - target
        rising_time = theta.index(max(theta))

        theta_reverse = theta[::-1]
        upper_stability_bound, lower_stability_bound = target + stability_bound, target - stability_bound
        settling_time = np.argmax(lower_stability_bound < theta_reverse < upper_stability_bound)
        settling_time = len(theta_reverse) - settling_time

        return (peak_overshot, rising_time, settling_time)


@gin.configurable
class RobustnessPlotter(Plotter):

    def __init__(self, env: PyEnvironment):
        super().__init__()
        self.env_name = env.unwrapped.spec.id[:-3]
        # Depending on the env, sin(theta) or state of interest is located at different obs_idx
        if self.env_name == 'Pendulum':
            self.asin = True
            self.acos = False
            self.obs_idx = 1
        elif self.env_name == 'Cartpole':
            self.asin = True
            self.acos = False
            self.obs_idx = 3
        elif self.env_name == 'Mountaincar':
            self.asin = False
            self.acos = False
            self.obs_idx = 0
        else:
            raise Exception(f'--- Error: Wrong env in {self} ---')

    def __call__(self,
                 dict_of_dict_controller_name_scaling_trajectory: Dict[str, Dict[float, List[Trajectory]]]) -> None:
        fig, ax = plt.subplots(figsize=(6, 4))
        lns, labs = [], []
        colors = ['mediumblue', 'dodgerblue', 'firebrick', 'orange', 'green']
        labels = ['DDPG_baseline', 'DDPG_hybrid', 'PILCO_baseline', 'PILCO_hybrid', 'linear_ctrl']

        for i, (controller_name, dict_scaling_trajectory) in enumerate(
                dict_of_dict_controller_name_scaling_trajectory.items()):
            scale, overshoot, rise, settle = [], [], [], []
            for scaling, trajectory in dict_scaling_trajectory.items():
                self._helper(trajectory)
                theta = self.traj2theta(obs_idx=self.obs_idx, asin=self.asin)
                overshoot, rising_time, settling_time = self._obtain_metrics(theta)
                scale.append(scaling)
                overshoot.append(overshoot)
                rising_time.append(rising_time)
                settling_time.append(settling_time)
            ln = ax.plot(scale, rise, color=colors[i % len(colors)], label=controller_name)

        ax.set_xlabel('Timesteps')
        ax.set_ylabel(f'\u03B8 (\u00B0)')

        # Set legend and format
        labs = [l.get_label() for l in lns]
        ax.legend(lns, labs)
        ax.set_title(f'{self.env_name}')
        fig.show()
        pass

    def _obtain_metrics(self, theta, target: float = 0, stability_bound: float = (np.pi * 0.1) / 180):
        peak_overshot = max(abs(theta)) - target
        rising_time = theta.index(max(theta))

        theta_reverse = theta[::-1]
        upper_stability_bound, lower_stability_bound = target + stability_bound, target - stability_bound
        settling_time = np.argmax(lower_stability_bound < theta_reverse < upper_stability_bound)
        settling_time = len(theta_reverse) - settling_time
        return (peak_overshot, rising_time, settling_time)


class LearningCurvePlotter(object):
    """
    This class plots learning curves for the entire training session from
    a list of np.arrays

    Input:
        rewards: a dictionary of the rewards
        The content of the dictionary should be structured as such:
        dict['controller/env/model'] = [([interaction time for x-axis ... ], [eval rewards for y-axis ... ]) ... ]
        The dictionary can be loaded from a pickle file using the load_pickle function
    """

    def __init__(self, rewards: dict = None, pickle_path: str = None):
        self.rewards = rewards
        self.pickle_path = pickle_path
        if self.pickle_path:
            self.load_pickle()
        self.graphs_num = 3

    def load_pickle(self):
        # load pickle into memory
        if os.path.isfile(self.pickle_path) and os.access(self.pickle_path, os.R_OK):
            # checks if file exists
            with open(self.pickle_path, 'rb') as f:
                self.rewards = pickle.load(f)
        else:
            raise Exception('--- Error: No pickle file found! ---')

    def __call__(self) -> None:
        # Partition the data into lines, each line representing a controller
        lines = self.rewards.keys()
        all = {}
        averages = {}
        best = {}
        worst = {}
        xs = {}

        for key in self.rewards:
            # Put the data for each line into an ndarray. Put this ndarray into dict all.
            xs[key] = np.array(self.rewards[key][0][0])
            all[key] = np.array(self.rewards[key][0][1])
            if len(self.rewards[key]) == 1:
                all[key] = np.expand_dims(all[key], axis=0)
            for i in range(1, len(self.rewards[key])):
                all[key] = np.vstack((all[key], self.rewards[key][i][1]))

            # Compute average, best, worst
            best_reward, worst_reward = np.max(all[key]), np.min(all[key])
            averages[key] = np.mean(all[key], axis=0)
            best[key] = np.max(all[key], axis=0)
            worst[key] = np.min(all[key], axis=0)
            # Standardisation
            averages[key] = (averages[key] - best_reward) / (best_reward - worst_reward) + 1.0
            best[key] = (best[key] - best_reward) / (best_reward - worst_reward) + 1.0
            worst[key] = (worst[key] - best_reward) / (best_reward - worst_reward) + 1.0

        # Plot graph
        titles = ['Pendulum', 'Cartpole', 'Mountaincar']
        regex = [r'controllers/pendulum/.*', r'controllers/cartpole/.*', r'controllers/mountaincar/.*']
        colors = ['mediumblue', 'dodgerblue', 'firebrick', 'orange', 'green']

        fig, axs = plt.subplots(1, self.graphs_num, figsize=(self.graphs_num * 4, 4))

        for i in range(self.graphs_num):
            cur_axis = axs[i]
            j = 0

            # Plot learning curve for the current env
            for line in lines:
                if re.match(regex[i], line):
                    cur_axis.plot(xs[line], averages[line], color=colors[j], label=line)
                    cur_axis.fill_between(xs[line], worst[line], best[line], alpha=0.5, facecolor=colors[j])
                    j += 1

            # Set legend and format
            cur_axis.set_ylim(0.0, 1.0)
            cur_axis.set_xscale('log')
            cur_axis.set_xlabel('Interaction time (s)')
            if i == 0:
                cur_axis.set_ylabel('Standardised rewards \u00B1 ')
            cur_axis.legend()
            cur_axis.set_title(titles[i])

        plt.show()


class RegexDict(dict):

    def __init__(self):
        super().__init__()

    def __getitem__(self, item):
        for k, v in self.items():
            if re.match(k, item):
                return v
        raise KeyError
