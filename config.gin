# Trainer.env =...
# Trainer.controller = ...
# Set up controller

# Shared Definitions
import dao.envs
load_py_env.name = 'Pendulum-v7'
load_weight_matrix.env = @load_py_env()
pythonenv/singleton.constructor = @load_py_env
load_tf_py_env.env = @pythonenv/singleton()
pythontfenv/singleton.constructor = @load_tf_py_env

# PILCO definitions
LinearController.W = @load_weight_matrix()
LinearController.env = @pythontfenv/singleton()
RbfController.env = @pythontfenv/singleton()

pythonrbfcontroller/singleton.constructor = @RbfController
PILCOTrainer.saved_path = 'data/1'
PILCOTrainer.controller = @pythonrbfcontroller/singleton()
PILCOTrainer.env = @pythontfenv/singleton()

train_pilco.env = @pythonenv/singleton()
train_pilco.controller = @pythonrbfcontroller/singleton()
train_pilco.target = [1.,0.,0.]
train_pilco.weights = [2.,2.,0.3]
train_pilco.m_init = [-1.,0.,0.]
train_pilco.S_init = [0.01, 0.05, 0.01]

# DDPG exclusives
ddpgsingleton/singleton.constructor = @DDPG

DDPG.train_env = @pythontfenv/singleton()
DDPG.controller_location = [1., 0., 0.]
DDPG.linear_controller = @LinearControllerLayer()
# DDPG.linear_controller = None # Uncomment this and comment above to switch between normal DDPG and our upgraded version

DDPGTrainer.env = @pythontfenv/singleton()
DDPGTrainer.ddpg = @ddpgsingleton/singleton()
DDPGTrainer.saved_path = 'data/1'
LinearControllerLayer.W = @load_weight_matrix()
LinearControllerLayer.env = @pythontfenv/singleton()

train_ddpg.ddpg = @ddpgsingleton/singleton()
train_ddpg.replay_buffer = @ReplayBuffer()
train_ddpg.eval_env = @pythontfenv/singleton()
train_ddpg.num_iterations = 20000

ReplayBuffer.ddpg = @ddpgsingleton/singleton()
ReplayBuffer.train_env = @pythontfenv/singleton()
ReplayBuffer.replay_buffer_capacity = 100000
ReplayBuffer.initial_collect_steps = 1000
ReplayBuffer.collect_steps_per_iteration = 1